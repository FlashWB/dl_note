- [前言](#前言)
- [Normalization](#normalization)
  - [Normalization 定义](#normalization-定义)
  - [什么情况需要 Normalization](#什么情况需要-normalization)
  - [Data Normalization 方法](#data-normalization-方法)
  - [示例代码](#示例代码)
- [参考资料](#参考资料)

## 前言

一般机器学习任务其工作流程可总结为如下 `pipeline`。

![机器学习任务pipeline](./images/data_preprocess/ml_pipeline.png)

在工业界，**数据预处理**步骤对模型精度的提高的发挥着重要作用。对于机器学习任务来说，广泛的数据预处理一般有四个阶段（**视觉任务一般只需 `Data Transformation`**）: 数据清洗(Data Cleaning)、数据整合(Data Integration)、数据转换(Data Transformation)和数据缩减(Data Reduction)。

![steps_in_data_preprocessing](../images/../dl_alchemy/images/data_preprocess/steps_in_data_preprocessing.png)

1，`Data Cleaning` 数据清理是数据预处理步骤的一部分，通过填充缺失值、平滑噪声数据、解决不一致和删除​​异常值来清理数据。
2，`Data Integration` 用于将存在于多个源中的数据合并到一个更大的数据存储中，如数据仓库。例如，将来自多个医疗节点的图像整合起来，形成一个更大的数据库。
3，在完成 `Data Cleaning` 后，我们需要通过使用下述数据转换策略更改数据的值、结构或格式。
- `Generalization`: 使用概念层次结构将低级或粒度数据转换为高级信息。例如将城市等地址中的原始数据转化为国家等更高层次的信息。
- `Normalization`: 目的是将数字属性按比例放大或缩小以适合指定范围。`Normalization` 常见方法:
  - Min-max normalization
  - Z-Score normalization
  - Decimal scaling normalization

4，`Data Reduction` 数据仓库中数据集的大小可能太大而无法通过数据分析和数据挖掘算法进行处理。一种可能的解决方案是获得数据集的缩减表示，该数据集的体积要小得多，但会产生相同质量的分析结果。常见的数据缩减策略如下:
- `Data cube aggregation`
- `Dimensionality reduction`: **降维技术用于执行特征提取**。数据集的维度是指数据的属性或个体特征。该技术旨在减少我们在机器学习算法中考虑的冗余特征的数量。降维可以使用主成分分析（`PCA`）等技术来完成。
- `Data compression`: 通过使用编码技术，数据的大小可以显着减小。
- `Discretization`: 数据离散化用于**将具有连续性的属性划分为具有区间的数据**。这样做是因为连续特征往往与目标变量相关的可能性较小。例如，属性年龄可以离散化为 18 岁以下、18-44 岁、44-60 岁、60 岁以上等区间。

对于计算机视觉任务来说，在训练 `CNN` 模型之前，对于输入数据特征做归一化（`normalization`）预处理（`data preprocessing`）操作是最常见的步骤。


## Normalization

> 这里没有翻译成中文，是因为目前中文翻译有些歧义，根据我查阅的博客资料，翻译为“归一化”比较多，仅供可参考。

### Normalization 定义

`Normalization` 操作被用于对数据属性进行缩放，使其落在较小的范围之内（即变化到某个固定区间中），比如 [-1,1] 和 [0, 1]，简单理解就是**特征缩放**过程。很多机器学习算法都受益于 `Normalization` 操作，比如:

- 通常对分类算法有用。
- 在梯度下降等机器学习算法的核心中使用的优化算法很有用。
- 对于加权输入的算法（如回归和神经网络）以及使用距离度量的算法（如 K 最近邻）也很有用。

### 什么情况需要 Normalization

当我们处理的**数据具有不同尺度（范围）**（`different scale`）时，通常就需要进行 `normalization` 操作了，它可能会导致一个重要属性（在较低尺度上）的有效性被稀释，因为其他属性具有更大范围（尺度）的值，简单点理解就是范围（`scale`）大点属性在模型当中更具优先级，具体示例如下图所示。

![different scale](../images/../dl_alchemy/images/data_preprocess/diffenent_scale.png)

总而言之，就是当数据存在多个属性但其值具有不同尺度（`scale`）时，这可能会导致我们在做数据挖掘操作时数据模型表现不佳，因此执行 `normalization` 操作将所有属性置于相同的尺寸内是很有必要的。

### Data Normalization 方法

1，`z-Score Normalization`

zero-mean Normalization，有时也称为 standardization，将数据特征缩放成均值为 0，方差为 1 的分布，对应公式: 

$$
{x}' = \frac{x-mean(x)}{\sigma}
$$

其中 $mean(x)$（有些地方用 $\mu =\frac{1}{N}\sum_{i=1}^{N} x_i$） 表示变量 $x$ 的均值，$\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N}(x_i - \mu)^2}$ 表示变量的标准差，${x}'$ 是数据缩放后的新值。

2，`Min-Max Normalization`

执行线性操作，将数据范围缩放到 $[0，1]$ 区间内，对应公式: 

$$
{x}' = \frac{x - min(x)}{max(x) - min(x)}
$$

其中 $max(x)$ 是变量最大值，$min(x)$ 是变量最小值。

### 示例代码

1，以下是使用 Python 和 Numpy 库实现 `Min-Max Normalization` 的示例代码：

```python
# 导入必要的库
import numpy as np
# 定义数据集
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
# 计算数据集的最小值和最大值
Xmin = X.min()
Xmax = X.max()
# 计算最小-最大规范化
X_norm = (X - Xmin) / (Xmax - Xmin)
# 打印结果
print(X_norm)
```

程序输出结果如下，可以看出原始数组数据都被缩放到 $[0, 1]$ 范围内了。

![程序输出结果](./images/data_preprocess/output2.png)

## 参考资料

- [Data Normalization in Data Mining](https://www.geeksforgeeks.org/data-normalization-in-data-mining/?ref=rp)
- 《解析卷积神经网络-第6章》
- [scikit-learn-6.3. Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)