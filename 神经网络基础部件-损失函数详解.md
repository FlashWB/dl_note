## 一，损失函数概述

大多数深度学习算法都会涉及某种形式的优化，**所谓优化指的是改变 $x$ 以最小化或最大化某个函数 $f(x)$ 的任务**，我们通常以最小化 $f(x)$ 指代大多数最优化问题。

在机器学习中，损失函数是代价函数的一部分，而代价函数是目标函数的一种类型。
- **损失函数**（`loss function`）: 用于定义单个训练样本预测值与真实值之间的误差
- **代价函数**（`cost function`）: 用于定义单个批次/整个训练集样本预测值与真实值之间的累计误差。
- **目标函数**（`objective function`）: 泛指任意可以被优化的函数。

**损失函数定义**：损失函数是深度学习模型训练过程中关键的一个组成部分，其通过前言的内容，我们知道深度学习算法优化的第一步首先是确定目标函数形式。

损失函数大致可分为两种：回归损失（针对连续型变量）和分类损失（针对离散型变量）。

常用的减少损失函数的优化算法是“梯度下降法”（Gradient Descent）。

## 二，分类损失

### 2.1，交叉熵（Cross-Entropy）

交叉熵损失(`Cross-Entropy Loss`) 又称为对数似然损失(Log-likelihood Loss)、对数损失；二分类时还可称之为逻辑斯谛回归损失(Logistic Loss)。

和 KL 散度一样，对于一个随机变量 $X$，我们也可以通过交叉熵来度量估计分布 $Q$ 和真实分布 $P$ 之间的差异（`divergence`）:

$$H(P, Q)  = -\mathbb{E}_{\textrm{x}\sim p}log(q(x))$$

使用信息论中熵的性质，交叉熵也可以定义为熵 $H(P)$ 和 $P$ 和 $Q$之间的 KL 散度的总和:

$$
CE(P, Q) = H(P) + D_{KL}(P||Q)
$$
## 三，回归损失

## 参考资料

- [《动手学深度学习-22.11. Information Theory》](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#cross-entropy)