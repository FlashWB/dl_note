> 本文内容参考资料为《深度学习》和《解析卷积神经网络》两本书，以及部分网络资料。

## 前言

我们知道神经网络模型一般是依靠**随机梯度下降**优化算法进行模型训练和模型参数更新的，模型的最终性能与收敛得到的最优解直接相关，而收敛效果实际上又很大程度取决于网络参数最开始初始化。

## 一，网络参数初始化概述

首先得明确的是现代的**网络参数初始化策略**是简单的、启发式的。设定改进的初始化策略是一项困难的 任务，因为神经网络优化至今还未被很好地理解（即模型训练过程是一个黑盒）。

大多数初始化策略基于在神经网 络初始化时实现一些很好的性质。然而，我们并没有很好地理解这些性质中的哪些会在学习开始进行后的哪些情况下得以保持。进一步的难点是，有些初始点从优化的观点看或许是有利的，但是从泛化的观点看是不利的。我们对于初始点如何影响泛化的理解是相当原始的，几乎没有提供如何选择初始点的任何指导。

目前也许完全确知的唯一特性是初始参数需要在不同单元间 **“破坏对称性”**。因为如果不同神经元的初始化参数相同，不同神经元的输出也必然相同，相同输出则导致梯度更新完全一样，即更新后的参数依然和之前一样，无法进行模型训练了。
### 1.1，进行网络参数初始化的原因

深度学习模型（神经网络模型）的训练算法通常是**迭代**的，因此模型训练者需要指定开始迭代的初始点，即择网络参数初始化策略。

### 1.1，网络参数初始化为什么重要

训练深度学习模型是一个足够困难的问题，以至于大多数算法都很大程度受到网络初始化策略的影响。

迭代的初始点能够决定算法是否收敛，有些初始点十分不稳定，使得该 算法会遭遇数值困难，并完全失败。当学习收敛时，初始点可以决定学习收敛得多快，以及是否收敛到一个代价高或低的点。另外，即使是具有同一个损失代价的迭代点也会有差别极大的泛化误差，而迭代初始点也可以影响泛化（误差）。

### 1.3，有效的初始化策略

在实际应用中，模型权重参数服从**高斯分布**（`Gaussian distribution`）或**均匀分布**（`uniform distribution`）都是较为**有效**的初始化方式。值得注意的是，这两种分布选择的区别，目前还没有被详尽的研究清楚，能够确定只有，初始分布的大小确实对优化过程的结果和网络泛化能力都有很大的影响。
> 权重初始化随机值的初始化策略的分布都对模型性能有影响，但是影响的原理（神经网络优化原理）又没有被彻底研究清楚，所谓模型训练，还真不愧是炼丹，部分时候还真得靠炼丹人的技巧。


## 参考资料

- 《深度学习-8.4 参数初始化策略》
- 《解析卷积神经网络-章 7 网络参数初始化》